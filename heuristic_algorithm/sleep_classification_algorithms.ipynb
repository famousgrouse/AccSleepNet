{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Classification Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will define some of the most common sleep algorithms used to date (excluding deep learning approches which are presented in a separate file). (This is an exploratory notebook and not the finalized one we use for our batch processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import pampro \n",
    "from datetime import datetime, date, time, timedelta\n",
    "from pampro import data_loading, Time_Series, Channel, channel_inference, Bout , triaxial_calibration\n",
    "from io import StringIO # required by pd.read_csv. It expects an object with a .read() method.\n",
    "from glob import glob\n",
    "import re \n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split, cross_val_predict, KFold, LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step on our journey to classify sleep-wake cycles is to define functions that are able to input all sorts of accelerometer and heart rate data.\n",
    "The current version of this software focuses on accelerometer data but next iterations will integrate HR signals (TO BE ADDED)\n",
    "\n",
    "Additionally, we will add definition functions for our ground truths (Polysomnography (PSG) gold standard method, sleep diaries, etc).\n",
    "\n",
    "** Potentially add an attribute or feature for age group to make sure we are using the best classifier or that we are not missinterpretting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerometer Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Accelerometer Data Preparation section we rely on PAMPRO: https://github.com/Thomite/pampro, which was developed by Tom White (@Thomite on Github) from our lab. We focus on Axivity, Actigraph, Geneactiv and ActivPAL data preprocessing here as they are the most common accelerometers used for research purposes these days but feel free to add any commits. The Preprocessing Steps done with PAMPRO are defined in sleep_data_preprocessing.py in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_acc_data(filename):\n",
    "    # load data\n",
    "    acc_df = pd.read_hdf(filename) # index_col=\"line\"?\n",
    "    # Check Activity\n",
    "    acc_df[\"ActivityValue\"] = acc_df[\"activity\"]\n",
    "    acc_df[\"NonActive\"] = acc_df[\"activity\"]== 0\n",
    "    #Timestamps\n",
    "    # get timestamps from Pampro here\n",
    "    \n",
    "    # add other potential features of use here\n",
    "    \n",
    "    \n",
    "    #del unecessary features here\n",
    "    \n",
    "    return acc_df\n",
    "\n",
    "filename = \"sleepdata/actigraphy/examplefilename001.csv\"\n",
    "\n",
    "acc_df = load_acc_data(filename)\n",
    "\n",
    "acc_df.head()\n",
    "# Check how it looks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time periods for Cole-Kripke, Sadeh and Saznov algorithms\n",
    "\n",
    "def get_start_period(header):\n",
    "    \n",
    "    start_date = header[3].split(\"Start Date\")[1].strip()\n",
    "    start_time = header[2].split(\"Start Time\")[1].strip()\n",
    "    \n",
    "    return start_time, start_date\n",
    "\n",
    "def get_timestamp(start_date, start_time):\n",
    "    return datetime.strptime(start_date + \" \" + start_time, '%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "def get_time_interval(n):\n",
    "    minutes = n / 2\n",
    "    hours = minutes/ 60\n",
    "    rest_minutes = minutes - (hours * 60)\n",
    "    rest_seconds = \"30\" if n%2 == 1 else \"00\"\n",
    "    return \"%02d:%02d:%s\" % (hours, rest_minutes, rest_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data from accelerometer for algorithm use\n",
    "# Adapted from Aarti's and Actigraph code\n",
    "\n",
    "def from_file_to_acc_df(filename):\n",
    "\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        header = lines[:10]\n",
    "    \n",
    "        start_time, start_date = get_start_period(header)\n",
    "\n",
    "        csv = io.StringIO(\"\".join(lines[10:]))\n",
    "      \n",
    "    acc_df = pd.read_csv(csv)\n",
    "\n",
    "    # Annotate data with time information\n",
    "    ts = get_timestamp(start_date, start_time)\n",
    "    pts = pd.Timestamp(ts)\n",
    "    acc_df[\"time\"] = pd.date_range(pts, periods=acc_df.shape[0], freq='S')\n",
    "\n",
    "    \n",
    "    #https://actigraph.desk.com/customer/en/portal/articles/2515585-where-can-i-find-documentation-for-the-sadeh-and-cole-kripke-algorithms-\n",
    "    # The maximal value for each axis is forced to be 300\n",
    "    #acc_df[[\"Axis1\",\"Axis2\",\"Axis3\"]] = acc_df[[\"Axis1\",\"Axis2\",\"Axis3\"]].clip(upper=300) \n",
    "    acc_df[[\"X\",\"Y\",\"Z\"]] = acc_df[[\"X\",\"Y\",\"Z\"]].clip(upper=300) \n",
    "\n",
    "    \n",
    "    # Group rows by minute\n",
    "    #df = df.resample('1Min', on=\"time\").mean().reset_index()\n",
    "    acc_df = acc_df.resample('1Min', on=\"time\").sum().reset_index()\n",
    "\n",
    "    # Add column to check for activity\n",
    "    #acc_df[\"NonActive\"] = acc_df[[\"Axis1\",\"Axis2\",\"Axis3\"]].apply(sum, axis=1) == 0\n",
    "    acc_df[\"NonActive\"] = acc_df[[\"X\",\"Y\",\"Z\"]].apply(sum, axis=1) == 0\n",
    "\n",
    "    \n",
    "    acc_df[\"actValue\"] = acc_df[\"X\"]\n",
    "    #acc_df[\"actValue\"] = acc_df[\"Axis1\"]\n",
    "\n",
    "    return acc_df\n",
    "\n",
    "filename = \"sleepdata/accelerometer/exampleacc001.csv\"\n",
    "\n",
    "acc_df = from_file_to_acc_df(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and deal with training data\n",
    "This data comes from either PSG, reduced array or diary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSG loading Adapted from Aarti's project:\n",
    "\n",
    "\n",
    "def  load_PSG(filename):\n",
    "    PSG_df = pd.read_csv(filename, index_col=\"line\")\n",
    "    \n",
    "    PSG_df[\"NonActive\"] = PSG_df[\"activity\"] == 0\n",
    "    PSG_df[\"ActivityValue\"] = PSG_df[\"activity\"]\n",
    "    PSG_df[\"time\"] = pd.to_datetime(PSG_df[\"linetime\"])\n",
    "    PSG_df[\"gt\"] = PSG_df[\"stage\"] > 0\n",
    "    \n",
    "    del PSG_df[\"linetime\"]\n",
    "    \n",
    "    return PSG_df\n",
    "\n",
    "filename = \"sleepdata/PSGdata/PSGnight_001.csv\"\n",
    "PSG_df = load_PSG(filename)\n",
    "\n",
    "PSG_df = PSG_df[PSG_df[\"interval\"] != \"EXCLUDED\"]\n",
    "PSG_df[\"active\"] = (PSG_df[\"interval\"] == \"ACTIVE\").astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced Array loading\n",
    "\n",
    "def  load_RedArray (filename):\n",
    "    RedArray_df = pd.read_csv(filename, index_col=\"line\")\n",
    "    \n",
    "    RedArray_df[\"NonActive\"] = RedArray_df[\"activity\"] == 0\n",
    "    RedArray_df[\"ActivityValue\"] = RedArray_df[\"activity\"]\n",
    "    RedArray_df[\"time\"] = pd.to_datetime(PSG_df[\"linetime\"])\n",
    "    RedArray_df[\"gt\"] = RedArray_df[\"stage\"] > 0\n",
    "    \n",
    "    del RedArray_df[\"linetime\"]\n",
    "    \n",
    "    return RedArray_df\n",
    "\n",
    "filename = \"sleepdata/ReducedArraydata/RedArray_001.csv\"\n",
    "RedArray_df = load_RedArray(filename)\n",
    "\n",
    "RedArray_df = RedArray_df[RedArray_df[\"interval\"] != \"EXCLUDED\"]\n",
    "RedArray_df[\"active\"] = (RedArray_df[\"interval\"] == \"ACTIVE\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Data Loading from Diary (Complexity here is different diary types will include some fields and some will not). \n",
    "# Use conditional statements for those cases (only real required fields are time in bed, time awake, all other should be optional)\n",
    "\n",
    "\n",
    "#//////////////////////////////////////////////////ADD DIARY PROCESSING HERE//////////////////\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first set of algorithms will include some of the best cited algorithms for actigraphy sleep-wake classification using the so-called heuristic approaches. These algorithms include the Sadeh, Cole-Kripke, Saznov and Van Hees algorithm.\n",
    "\n",
    "(Potentially add others in the near future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Sadeh's algorithm using 6 windows prior and centered on the the middle of 11 total windows. We follow: #https://actigraph.desk.com/customer/en/portal/articles/2515585-where-can-i-find-documentation-for-the-sadeh-and-cole-kripke-algorithms-\n",
    "\n",
    "def sadeh(acc_df, min_value=0):\n",
    "    window_past = 6\n",
    "    window_nat = 11\n",
    "    window_centered = 11\n",
    "    \n",
    "    acc_df[\"_mean\"] = acc_df[\"ActivityValue\"].rolling(window=window_centered, center=True, min_periods=1).mean()\n",
    "    acc_df[\"_std\"] = acc_df[\"ActivityValue\"].rolling(window=window_past, min_periods=1).std()\n",
    "    acc_df[\"_nat\"] = ((acc_df[\"ActivityValue\"] >= 50) & (acc_df[\"actValue\"] < 100)).rolling(window=window_nat, center=True, min_periods=1).sum()\n",
    "    \n",
    "    df[\"_LocAct\"] = (acc_df[\"ActivityValue\"] + 1.).apply(np.log) \n",
    "    \n",
    "    acc_df[\"sadeh\"] = (7.601 - 0.065 * acc_df[\"_mean\"] - 0.056 * acc_df[\"_std\"] - 0.0703 *acc_df[\"_LocAct\"] - 1.08 * acc_df[\"_nat\"])\n",
    "    acc_df[\"sadeh\"] = (acc_df[\"sadeh\"]  > min_value).astype(int)\n",
    "\n",
    "    del acc_df[\"_mean\"]\n",
    "    del acc_df[\"_std\"]\n",
    "    del acc_df[\"_nat\"]\n",
    "    del acc_df[\"_LocAct\"]\n",
    "\n",
    "sadeh(acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cole-Kripke algorithm, again following ActiGraph's site instructions\n",
    "def cole(acc_df):\n",
    "    acc_df[\"_A0\"] = acc_df[\"ActivityValue\"]\n",
    "    for i in range(1,5):\n",
    "        acc_df[\"_A-%d\" % (i)] = acc_df[\"ActivityValue\"].shift(i).fillna(0.0)\n",
    "    for i in range(1,3):\n",
    "        acc_df[\"_A+%d\" % (i)] = acc_df[\"ActivityValue\"].shift(-i).fillna(0.0)\n",
    "\n",
    "    w_m4, w_m3, w_m2, w_m1, w_0, w_p1, w_p2 = [404, 598, 326, 441, 1408, 508, 350]\n",
    "    p = 0.00001\n",
    "    \n",
    "    cole = p * (w_m4 * acc_df[\"_A-4\"] + w_m3 * acc_df[\"_A-3\"] + w_m2 * acc_df[\"_A-2\"] + w_m1 * acc_df[\"_A-1\"] + w_0 * acc_df[\"_A0\"] + w_p1 * acc_df[\"_A+1\"] + w_p2 * acc_df[\"_A+2\"])\n",
    "    acc_df[\"cole\"] = (cole < 1.0).astype(int)\n",
    "        \n",
    "cole(acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saznov Sleep Classification Algorithm (DERIVED IN INFANTS), adapted from TILMANNES paper: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2869.2008.00706.x\n",
    "# which reflects on :https://www.semanticscholar.org/paper/Activity-based-sleep-wake-identification-in-Sazonov-Sazonova/1b152434c886024a8a2b20b31121abd9af953e0f\n",
    "\n",
    "def saznov(acc_df):\n",
    "    for w in range(1,10):\n",
    "        acc_df[\"_w%d\" % (w-1)] = acc_df[\"ActivityValue\"].rolling(window=w, min_periods=1).max()\n",
    "\n",
    "    acc_df[\"saznov\"] = 1.99604  - 0.1945 * acc_df[\"_w0\"]- 0.09746 * acc_df[\"_w1\"]- 0.09975 * acc_df[\"_w2\"]- 0.10194 * acc_df[\"_w3\"]\\\n",
    "                            - 0.08917 * acc_df[\"_w4\"]- 0.08108 * acc_df[\"_w5\"]- 0.07494 * acc_df[\"_w6\"]- 0.07300 * acc_df[\"_w7\"]\\\n",
    "                            - 0.10207 * acc_df[\"_w8\"]\n",
    "                            \n",
    "    for w in range(1,10):\n",
    "        del acc_df[\"_w%d\" % (w-1)]\n",
    "\n",
    "    acc_df[\"saznov\"] = (acc_df[\"saznov\"]  > 0.0).astype(int)  \n",
    "saznov(acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Van Hees Algorithm (2018, SPT based) in Python here. Adapt from R code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple methods have shown different levels of success, we will explore Naive Bayes, Regularized Logistic Regression, Random Forrest, Adaboost and Extreme Gradient Boosting (Potentially include only XGBoost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the last 5 years, the resurgence of artificial neural networks by improved computing capabilities through the use of GPUs has popularized these approaches in time-series inferences, we present a separate pipeline for deep learning approaches in our repository under sleep_classification_deep. Please make sure you have installed the appropriate libraries and packages and have set up either a GPU rig or cloud computing services for these purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
